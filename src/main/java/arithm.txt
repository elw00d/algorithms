Часто возникающие вопросы:

1) Как подсчитать встречаемости символов ? Какая максимальная суммарная встречаемость символов
   допустима в кодере при N битах ?

2) Какими битами нужно завершить кодирование и почему ?

3) Почему формула для получения threshold именно такая ?

int threshold = ( int ) ((((value & 0xFFFFFFFFL) - left + 1) * totalCount - 1) / range);

Для того, чтобы декодировать очередной символ, можно
вычислять для каждого тестируемого символа low' и high', проверяя неравенство
low' <= value <= high' на истинность:

int c;
for(c = 0; c < alphabetSize; c++){
    int left_ = ( int ) (left + sumProbs[c] * (range / totalCount) );
    int right_ = ( int ) (left + (sumProbs[c] + probs[c]) * (range / totalCount) - 1);
    if(compareUnsigned(value, left_) >= 0 && compareUnsigned(value, right_) <= 0){
        break;
    }
}

Этот цикл можно оптимизировать, вычисляя сначала только left', потом right'. Можно также
воспользоваться двоичным поиском, но в любом случае это будет медленнее, чем вычисление
искомой накопленной встречаемости и дальнейший поиск подходящего символа:

int threshold = ( int ) ((((value & 0xFFFFFFFFL) - left + 1) * totalCount - 1) / range);

Здесь мы берём размер интервала [left; value), умножаем этот размер на totalCount и делим на range.
Необходимость сначала добавить единицу, а потом отнять может вызвать недоумение, однако при
работе с интервалами, представляемыми в двоичном виде, это необходимо. Пояснением может служить
следующий пример: у нас есть интервал [0; 64). В целых числах он представляется как [0; 63].
При умножении на 2 он должен превратиться в [0; 128). Но 63 * 2 = 126. Поэтому перед операциями
над размерами интервалов нужно прибавить единицу к верхней границе, а после - отнять. На выходе
мы получаем корректную верхнюю границу нового интервала. Таким образом, здесь выполняется деление
верхней границы интервала [left, value) на размер полного рабочего интервала, на выходе дающий нам значение
0 <= threshold < totalCount, которое однозначно идентифицирует закодированный символ.

Ну а операция & 0xFFFFFFFFL это не что иное как преобразование value в long перед умножением,
которое может привести к переполнению, если выполнять его в разрядности нашего кодера.

В оригинальной pdf http://www.stanford.edu/class/ee398a/handouts/papers/WittenACM87ArithmCoding.pdf
даётся формальное доказательство правильности декодирования с использованием этой формулы.
Действительно, её довольно трудно понять, так как механизм её работы не совем очевиден из-за использования
целочисленного деления, которое непривычно для нашего мозга.

4) Почему мы используем разрядность 32, если современные компьютеры наиболее эффективно
работают с 64-битными числами ? Можно ли в java сделать 64-битовый кодер ?

Думаю, что можно, но для этого понадобится написать свои реализации умножения и беззнакового деления.
Умножение необходимо 128-битное, так как если мы будем использовать 64 бита, то при умножении
двух 64-битных чисел рано или поздно мы столкнёмся с переполнением. Беззнаковое деление 64-битных чисел -
это чисто джавская проблема, поскольку java не поддерживает беззнаковые типы данных. При использовании
32-битных чисел мы обходились переходом к long для деления, в 64-битном режиме это уже не прокатит.
На самом деле в guava уже есть реализация (написанная максимально эффективно), её можно просто выдрать
оттуда. Но что делать с умножением - непонятно.

5) Изменится ли что-то, если выполнять сначала деление, а потом умножение ? В формулах вычисления
следующих значений границ интервала * range / totalCount - ведь у нас в любом случае totalCount <= range,
и мы имеем право это сделать ? Это даст нам возможность перейти к 64-битной арифметике.

Да, изменится - точность уменьшится, а значит, и степень сжатия:

Source size 1473547 encoded size 896750 ratio 60,856559%
Source size 1473547 encoded size 896834 ratio 60,862260%

Почему уменьшается точность ? Допустим, у нас totalCount = 256, а range = 500. И множитель,
на который мы будем умножать накопленную встречаемость очередного символа, будет равен 256 div 500 = 1.
Так как максимальная накопленная встречаемость у нас = 256, то по сути мы используем только первые
256 возможных чисел, хотя размер интервала сейчас равен 500. Это эквивалентно тому, что мы сами себе
уменьшили размер рабочего интервала без нужды (этот приём как раз успешно применяется для избегания
обработки переносов). Поэтому мы должны будем потратить больше бит для уточнения дальнейшего
местоположения числа. Потерь точности не возникает только тогда, когда range делится на totalCount нацело
без остатка.

Неприятным будет ещё и то, что формула

int threshold = ( int ) ((((value & 0xFFFFFFFFL) - left + 1) * totalCount - 1) / range);

перестанет работать. Но так как мы знаем, как теряется точность из-за сужения интервала, мы
можем её скорректировать:

int threshold = ( int ) ((((value & 0xFFFFFFFFL) - left + 1) * totalCount - 1)
                    / (range - range % totalCount));

TODO : Проверить, равносильно ли это формуле

int threshold = (value - left) / (range / totalCount);

Здесь мы делим уже не на полный range, а на ту часть range, которая действительно была использована при
кодировании этого символа. В нашем примере это будет 256 вместо 500.

Скорее всего, такая потеря точности не будет оправдана переходом к 64 битам. Впрочем,
можно попробовать.